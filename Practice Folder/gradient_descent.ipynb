{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c530bdd4",
   "metadata": {},
   "source": [
    "### 1. Prepare Data\n",
    "#### 1.1 Get your X and y in the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "31a51132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "99d9900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "02e3f432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "e47c2e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi  ...        s4        s5        s6\n",
       "0  0.038076  0.050680  0.061696  ... -0.002592  0.019907 -0.017646\n",
       "1 -0.001882 -0.044642 -0.051474  ... -0.039493 -0.068332 -0.092204\n",
       "2  0.085299  0.050680  0.044451  ... -0.002592  0.002861 -0.025930\n",
       "3 -0.089063 -0.044642 -0.011595  ...  0.034309  0.022688 -0.009362\n",
       "4  0.005383 -0.044642 -0.036385  ... -0.002592 -0.031988 -0.046641\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "e5fcc3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "94474adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp  ...        s4        s5        s6  target\n",
       "0  0.038076  0.050680  0.061696  0.021872  ... -0.002592  0.019907 -0.017646   151.0\n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328  ... -0.039493 -0.068332 -0.092204    75.0\n",
       "2  0.085299  0.050680  0.044451 -0.005670  ... -0.002592  0.002861 -0.025930   141.0\n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  ...  0.034309  0.022688 -0.009362   206.0\n",
       "4  0.005383 -0.044642 -0.036385  0.021872  ... -0.002592 -0.031988 -0.046641   135.0\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "27f7c4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age       0\n",
       "sex       0\n",
       "bmi       0\n",
       "bp        0\n",
       "s1        0\n",
       "s2        0\n",
       "s3        0\n",
       "s4        0\n",
       "s5        0\n",
       "s6        0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "0bc67d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "e77def2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "3515cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "0e256904",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "1d50d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scalling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "b9c3ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BGD:\n",
    "    def __init__(self, max_iter=10000, lr=0.01, tol=0.1):\n",
    "        self.theta = None\n",
    "        self.max_iter = max_iter\n",
    "        self.lr = lr\n",
    "        self.tol = tol   # tolerance to stop early\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        intercept = np.ones((X_train.shape[0],1))\n",
    "        X_train = np.concatenate((intercept,X_train), axis = 1)\n",
    "        if self.theta is None:\n",
    "            self.theta = np.zeros(X_train.shape[1])\n",
    "        \n",
    "        prev_loss = float('inf')\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            y_hat = X_train @ self.theta\n",
    "            grad = (X_train.T @ (y_hat - y_train))/(X_train.shape[0])\n",
    "            self.theta = self.theta - (self.lr * grad)\n",
    "            loss = np.mean((y_hat - y_train) ** 2)\n",
    "            print(f\"Iteration {i}: loss = {loss}\")\n",
    "            if abs(prev_loss - loss) < self.tol:\n",
    "                print(f\"Stopped early at iteration {i}, loss={loss}\")\n",
    "                break\n",
    "            prev_loss = loss\n",
    "        \n",
    "        return self.theta\n",
    "    def predict(self,X_test):\n",
    "        intercept = np.ones((X_test.shape[0],1))\n",
    "        X_test = np.concatenate((intercept,X_test), axis = 1)\n",
    "        y_hat = X_test @ self.theta\n",
    "     \n",
    "        return y_hat\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "59d5df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BGD()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "48656423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 29711.32294617564\n",
      "Iteration 1: loss = 29067.676124881225\n",
      "Iteration 2: loss = 28445.25829278914\n",
      "Iteration 3: loss = 27842.996629427085\n",
      "Iteration 4: loss = 27259.89012384567\n",
      "Iteration 5: loss = 26695.004223620414\n",
      "Iteration 6: loss = 26147.46589536569\n",
      "Iteration 7: loss = 25616.459064843253\n",
      "Iteration 8: loss = 25101.220407228815\n",
      "Iteration 9: loss = 24601.03546038858\n",
      "Iteration 10: loss = 24115.235036127702\n",
      "Iteration 11: loss = 23643.191906318792\n",
      "Iteration 12: loss = 23184.317742613235\n",
      "Iteration 13: loss = 22738.060290093097\n",
      "Iteration 14: loss = 22303.90075674812\n",
      "Iteration 15: loss = 21881.35140206944\n",
      "Iteration 16: loss = 21469.953309350392\n",
      "Iteration 17: loss = 21069.274327481544\n",
      "Iteration 18: loss = 20678.9071691315\n",
      "Iteration 19: loss = 20298.467653223106\n",
      "Iteration 20: loss = 19927.593080553757\n",
      "Iteration 21: loss = 19565.94073227467\n",
      "Iteration 22: loss = 19213.186481742396\n",
      "Iteration 23: loss = 18869.023510992793\n",
      "Iteration 24: loss = 18533.161123766633\n",
      "Iteration 25: loss = 18205.323647642665\n",
      "Iteration 26: loss = 17885.249418411804\n",
      "Iteration 27: loss = 17572.689840358762\n",
      "Iteration 28: loss = 17267.408516609008\n",
      "Iteration 29: loss = 16969.180444152145\n",
      "Iteration 30: loss = 16677.791268570676\n",
      "Iteration 31: loss = 16393.036593888777\n",
      "Iteration 32: loss = 16114.721343311072\n",
      "Iteration 33: loss = 15842.65916694935\n",
      "Iteration 34: loss = 15576.67189293767\n",
      "Iteration 35: loss = 15316.589018614963\n",
      "Iteration 36: loss = 15062.247238711769\n",
      "Iteration 37: loss = 14813.490007714654\n",
      "Iteration 38: loss = 14570.167133801031\n",
      "Iteration 39: loss = 14332.134401938527\n",
      "Iteration 40: loss = 14099.253223929447\n",
      "Iteration 41: loss = 13871.39031335247\n",
      "Iteration 42: loss = 13648.417383511925\n",
      "Iteration 43: loss = 13430.210866651236\n",
      "Iteration 44: loss = 13216.651652821654\n",
      "Iteration 45: loss = 13007.624846921675\n",
      "Iteration 46: loss = 12803.019542537171\n",
      "Iteration 47: loss = 12602.728611317922\n",
      "Iteration 48: loss = 12406.64850672365\n",
      "Iteration 49: loss = 12214.679081062759\n",
      "Iteration 50: loss = 12026.72341482969\n",
      "Iteration 51: loss = 11842.68765742355\n",
      "Iteration 52: loss = 11662.480878401037\n",
      "Iteration 53: loss = 11486.014928481918\n",
      "Iteration 54: loss = 11313.2043095853\n",
      "Iteration 55: loss = 11143.96605323035\n",
      "Iteration 56: loss = 10978.21960668623\n",
      "Iteration 57: loss = 10815.886726303119\n",
      "Iteration 58: loss = 10656.891377499689\n",
      "Iteration 59: loss = 10501.159640922599\n",
      "Iteration 60: loss = 10348.61962433042\n",
      "Iteration 61: loss = 10199.201379788701\n",
      "Iteration 62: loss = 10052.836825794355\n",
      "Iteration 63: loss = 9909.459673976522\n",
      "Iteration 64: loss = 9769.005360048011\n",
      "Iteration 65: loss = 9631.410978706075\n",
      "Iteration 66: loss = 9496.615222204138\n",
      "Iteration 67: loss = 9364.558322337187\n",
      "Iteration 68: loss = 9235.181995602903\n",
      "Iteration 69: loss = 9108.429391318668\n",
      "Iteration 70: loss = 8984.24504249101\n",
      "Iteration 71: loss = 8862.574819249407\n",
      "Iteration 72: loss = 8743.3658846705\n",
      "Iteration 73: loss = 8626.566652831652\n",
      "Iteration 74: loss = 8512.126748944955\n",
      "Iteration 75: loss = 8399.996971433806\n",
      "Iteration 76: loss = 8290.129255824366\n",
      "Iteration 77: loss = 8182.476640333783\n",
      "Iteration 78: loss = 8076.993233045645\n",
      "Iteration 79: loss = 7973.63418057133\n",
      "Iteration 80: loss = 7872.3556381032295\n",
      "Iteration 81: loss = 7773.1147407727885\n",
      "Iteration 82: loss = 7675.869576232586\n",
      "Iteration 83: loss = 7580.579158387569\n",
      "Iteration 84: loss = 7487.20340220598\n",
      "Iteration 85: loss = 7395.70309954544\n",
      "Iteration 86: loss = 7306.039895934417\n",
      "Iteration 87: loss = 7218.176268253383\n",
      "Iteration 88: loss = 7132.075503264099\n",
      "Iteration 89: loss = 7047.701676938955\n",
      "Iteration 90: loss = 6965.0196345457625\n",
      "Iteration 91: loss = 6883.994971446491\n",
      "Iteration 92: loss = 6804.594014571261\n",
      "Iteration 93: loss = 6726.783804531645\n",
      "Iteration 94: loss = 6650.53207833976\n",
      "Iteration 95: loss = 6575.807252701897\n",
      "Iteration 96: loss = 6502.578407857559\n",
      "Iteration 97: loss = 6430.81527193673\n",
      "Iteration 98: loss = 6360.488205809995\n",
      "Iteration 99: loss = 6291.568188407793\n",
      "Iteration 100: loss = 6224.026802486676\n",
      "Iteration 101: loss = 6157.836220821784\n",
      "Iteration 102: loss = 6092.96919280621\n",
      "Iteration 103: loss = 6029.399031439029\n",
      "Iteration 104: loss = 5967.099600684993\n",
      "Iteration 105: loss = 5906.045303189883\n",
      "Iteration 106: loss = 5846.211068336569\n",
      "Iteration 107: loss = 5787.5723406276575\n",
      "Iteration 108: loss = 5730.105068381494\n",
      "Iteration 109: loss = 5673.785692729074\n",
      "Iteration 110: loss = 5618.591136900119\n",
      "Iteration 111: loss = 5564.49879578727\n",
      "Iteration 112: loss = 5511.486525777961\n",
      "Iteration 113: loss = 5459.532634844141\n",
      "Iteration 114: loss = 5408.615872880545\n",
      "Iteration 115: loss = 5358.7154222827\n",
      "Iteration 116: loss = 5309.810888756368\n",
      "Iteration 117: loss = 5261.882292350518\n",
      "Iteration 118: loss = 5214.910058706366\n",
      "Iteration 119: loss = 5168.875010515366\n",
      "Iteration 120: loss = 5123.758359179443\n",
      "Iteration 121: loss = 5079.541696667013\n",
      "Iteration 122: loss = 5036.206987558733\n",
      "Iteration 123: loss = 4993.736561277143\n",
      "Iteration 124: loss = 4952.113104494675\n",
      "Iteration 125: loss = 4911.319653714723\n",
      "Iteration 126: loss = 4871.3395880207645\n",
      "Iteration 127: loss = 4832.156621988651\n",
      "Iteration 128: loss = 4793.7547987575035\n",
      "Iteration 129: loss = 4756.118483254752\n",
      "Iteration 130: loss = 4719.232355571099\n",
      "Iteration 131: loss = 4683.081404481346\n",
      "Iteration 132: loss = 4647.650921107155\n",
      "Iteration 133: loss = 4612.9264927180275\n",
      "Iteration 134: loss = 4578.89399666687\n",
      "Iteration 135: loss = 4545.539594456685\n",
      "Iteration 136: loss = 4512.849725935052\n",
      "Iteration 137: loss = 4480.811103613171\n",
      "Iteration 138: loss = 4449.410707106354\n",
      "Iteration 139: loss = 4418.635777692989\n",
      "Iteration 140: loss = 4388.473812989078\n",
      "Iteration 141: loss = 4358.912561735515\n",
      "Iteration 142: loss = 4329.940018695466\n",
      "Iteration 143: loss = 4301.54441965917\n",
      "Iteration 144: loss = 4273.714236553661\n",
      "Iteration 145: loss = 4246.438172654962\n",
      "Iteration 146: loss = 4219.705157900348\n",
      "Iteration 147: loss = 4193.504344298394\n",
      "Iteration 148: loss = 4167.82510143455\n",
      "Iteration 149: loss = 4142.657012070096\n",
      "Iteration 150: loss = 4117.9898678323525\n",
      "Iteration 151: loss = 4093.8136649940793\n",
      "Iteration 152: loss = 4070.118600340122\n",
      "Iteration 153: loss = 4046.8950671193134\n",
      "Iteration 154: loss = 4024.1336510797846\n",
      "Iteration 155: loss = 4001.82512658583\n",
      "Iteration 156: loss = 3979.9604528145524\n",
      "Iteration 157: loss = 3958.530770030555\n",
      "Iteration 158: loss = 3937.5273959369724\n",
      "Iteration 159: loss = 3916.9418221012093\n",
      "Iteration 160: loss = 3896.765710453756\n",
      "Iteration 161: loss = 3876.9908898585427\n",
      "Iteration 162: loss = 3857.609352753281\n",
      "Iteration 163: loss = 3838.6132518583104\n",
      "Iteration 164: loss = 3819.9948969524944\n",
      "Iteration 165: loss = 3801.7467517147443\n",
      "Iteration 166: loss = 3783.8614306297995\n",
      "Iteration 167: loss = 3766.3316959568792\n",
      "Iteration 168: loss = 3749.1504547599184\n",
      "Iteration 169: loss = 3732.310755998082\n",
      "Iteration 170: loss = 3715.8057876752964\n",
      "Iteration 171: loss = 3699.6288740475657\n",
      "Iteration 172: loss = 3683.7734728868777\n",
      "Iteration 173: loss = 3668.2331728005192\n",
      "Iteration 174: loss = 3653.0016906046435\n",
      "Iteration 175: loss = 3638.072868750978\n",
      "Iteration 176: loss = 3623.440672805566\n",
      "Iteration 177: loss = 3609.099188978475\n",
      "Iteration 178: loss = 3595.042621703407\n",
      "Iteration 179: loss = 3581.2652912661974\n",
      "Iteration 180: loss = 3567.7616314811958\n",
      "Iteration 181: loss = 3554.526187414536\n",
      "Iteration 182: loss = 3541.5536131533363\n",
      "Iteration 183: loss = 3528.8386696199027\n",
      "Iteration 184: loss = 3516.376222429992\n",
      "Iteration 185: loss = 3504.1612397942445\n",
      "Iteration 186: loss = 3492.188790461912\n",
      "Iteration 187: loss = 3480.454041706006\n",
      "Iteration 188: loss = 3468.9522573490185\n",
      "Iteration 189: loss = 3457.678795828405\n",
      "Iteration 190: loss = 3446.629108301005\n",
      "Iteration 191: loss = 3435.798736785615\n",
      "Iteration 192: loss = 3425.183312342931\n",
      "Iteration 193: loss = 3414.778553292113\n",
      "Iteration 194: loss = 3404.5802634632155\n",
      "Iteration 195: loss = 3394.5843304847626\n",
      "Iteration 196: loss = 3384.786724105753\n",
      "Iteration 197: loss = 3375.183494551397\n",
      "Iteration 198: loss = 3365.770770911903\n",
      "Iteration 199: loss = 3356.5447595636415\n",
      "Iteration 200: loss = 3347.50174262203\n",
      "Iteration 201: loss = 3338.6380764255036\n",
      "Iteration 202: loss = 3329.9501900499363\n",
      "Iteration 203: loss = 3321.4345838528975\n",
      "Iteration 204: loss = 3313.0878280471434\n",
      "Iteration 205: loss = 3304.9065613027506\n",
      "Iteration 206: loss = 3296.8874893773113\n",
      "Iteration 207: loss = 3289.0273837736304\n",
      "Iteration 208: loss = 3281.3230804243613\n",
      "Iteration 209: loss = 3273.77147840304\n",
      "Iteration 210: loss = 3266.3695386609857\n",
      "Iteration 211: loss = 3259.1142827895465\n",
      "Iteration 212: loss = 3252.002791807174\n",
      "Iteration 213: loss = 3245.0322049708348\n",
      "Iteration 214: loss = 3238.1997186112594\n",
      "Iteration 215: loss = 3231.502584991556\n",
      "Iteration 216: loss = 3224.9381111887105\n",
      "Iteration 217: loss = 3218.5036579975203\n",
      "Iteration 218: loss = 3212.1966388565024\n",
      "Iteration 219: loss = 3206.01451879534\n",
      "Iteration 220: loss = 3199.954813403421\n",
      "Iteration 221: loss = 3194.015087819071\n",
      "Iteration 222: loss = 3188.1929557390285\n",
      "Iteration 223: loss = 3182.4860784477864\n",
      "Iteration 224: loss = 3176.892163866375\n",
      "Iteration 225: loss = 3171.4089656202223\n",
      "Iteration 226: loss = 3166.0342821256763\n",
      "Iteration 227: loss = 3160.765955694847\n",
      "Iteration 228: loss = 3155.6018716583644\n",
      "Iteration 229: loss = 3150.5399575057354\n",
      "Iteration 230: loss = 3145.5781820428956\n",
      "Iteration 231: loss = 3140.7145545666576\n",
      "Iteration 232: loss = 3135.947124055683\n",
      "Iteration 233: loss = 3131.273978377664\n",
      "Iteration 234: loss = 3126.693243512377\n",
      "Iteration 235: loss = 3122.2030827903054\n",
      "Iteration 236: loss = 3117.8016961464955\n",
      "Iteration 237: loss = 3113.487319389367\n",
      "Iteration 238: loss = 3109.2582234841534\n",
      "Iteration 239: loss = 3105.112713850691\n",
      "Iteration 240: loss = 3101.0491296752703\n",
      "Iteration 241: loss = 3097.065843236259\n",
      "Iteration 242: loss = 3093.1612592432225\n",
      "Iteration 243: loss = 3089.333814189282\n",
      "Iteration 244: loss = 3085.5819757164254\n",
      "Iteration 245: loss = 3081.9042419935254\n",
      "Iteration 246: loss = 3078.299141106805\n",
      "Iteration 247: loss = 3074.7652304625017\n",
      "Iteration 248: loss = 3071.3010962014837\n",
      "Iteration 249: loss = 3067.905352625579\n",
      "Iteration 250: loss = 3064.5766416353836\n",
      "Iteration 251: loss = 3061.313632179321\n",
      "Iteration 252: loss = 3058.1150197137163\n",
      "Iteration 253: loss = 3054.97952567368\n",
      "Iteration 254: loss = 3051.9058969545736\n",
      "Iteration 255: loss = 3048.8929054038426\n",
      "Iteration 256: loss = 3045.9393473230257\n",
      "Iteration 257: loss = 3043.04404297971\n",
      "Iteration 258: loss = 3040.2058361292616\n",
      "Iteration 259: loss = 3037.4235935461083\n",
      "Iteration 260: loss = 3034.696204564407\n",
      "Iteration 261: loss = 3032.0225806278904\n",
      "Iteration 262: loss = 3029.4016548487157\n",
      "Iteration 263: loss = 3026.8323815751337\n",
      "Iteration 264: loss = 3024.3137359678067\n",
      "Iteration 265: loss = 3021.8447135845945\n",
      "Iteration 266: loss = 3019.424329973637\n",
      "Iteration 267: loss = 3017.0516202745816\n",
      "Iteration 268: loss = 3014.7256388277733\n",
      "Iteration 269: loss = 3012.4454587912596\n",
      "Iteration 270: loss = 3010.210171765459\n",
      "Iteration 271: loss = 3008.0188874253163\n",
      "Iteration 272: loss = 3005.8707331598216\n",
      "Iteration 273: loss = 3003.7648537187356\n",
      "Iteration 274: loss = 3001.7004108663614\n",
      "Iteration 275: loss = 2999.6765830422523\n",
      "Iteration 276: loss = 2997.692565028689\n",
      "Iteration 277: loss = 2995.7475676248023\n",
      "Iteration 278: loss = 2993.8408173272132\n",
      "Iteration 279: loss = 2991.971556017048\n",
      "Iteration 280: loss = 2990.1390406532114\n",
      "Iteration 281: loss = 2988.3425429717804\n",
      "Iteration 282: loss = 2986.581349191417\n",
      "Iteration 283: loss = 2984.8547597246497\n",
      "Iteration 284: loss = 2983.1620888949356\n",
      "Iteration 285: loss = 2981.5026646593697\n",
      "Iteration 286: loss = 2979.8758283369275\n",
      "Iteration 287: loss = 2978.2809343421454\n",
      "Iteration 288: loss = 2976.7173499241167\n",
      "Iteration 289: loss = 2975.184454910696\n",
      "Iteration 290: loss = 2973.6816414578166\n",
      "Iteration 291: loss = 2972.2083138038124\n",
      "Iteration 292: loss = 2970.7638880286527\n",
      "Iteration 293: loss = 2969.3477918179674\n",
      "Iteration 294: loss = 2967.9594642318066\n",
      "Iteration 295: loss = 2966.598355477999\n",
      "Iteration 296: loss = 2965.263926690048\n",
      "Iteration 297: loss = 2963.9556497094504\n",
      "Iteration 298: loss = 2962.673006872376\n",
      "Iteration 299: loss = 2961.4154908005894\n",
      "Iteration 300: loss = 2960.18260419656\n",
      "Iteration 301: loss = 2958.973859642651\n",
      "Iteration 302: loss = 2957.788779404325\n",
      "Iteration 303: loss = 2956.626895237273\n",
      "Iteration 304: loss = 2955.4877481983926\n",
      "Iteration 305: loss = 2954.370888460546\n",
      "Iteration 306: loss = 2953.275875131005\n",
      "Iteration 307: loss = 2952.2022760735235\n",
      "Iteration 308: loss = 2951.149667733965\n",
      "Iteration 309: loss = 2950.117634969405\n",
      "Iteration 310: loss = 2949.1057708806447\n",
      "Iteration 311: loss = 2948.1136766480645\n",
      "Iteration 312: loss = 2947.1409613707606\n",
      "Iteration 313: loss = 2946.187241908882\n",
      "Iteration 314: loss = 2945.252142729116\n",
      "Iteration 315: loss = 2944.33529575326\n",
      "Iteration 316: loss = 2943.436340209814\n",
      "Iteration 317: loss = 2942.554922488525\n",
      "Iteration 318: loss = 2941.6906959978432\n",
      "Iteration 319: loss = 2940.8433210252197\n",
      "Iteration 320: loss = 2940.012464600186\n",
      "Iteration 321: loss = 2939.19780036017\n",
      "Iteration 322: loss = 2938.3990084189836\n",
      "Iteration 323: loss = 2937.6157752379363\n",
      "Iteration 324: loss = 2936.8477934995226\n",
      "Iteration 325: loss = 2936.094761983618\n",
      "Iteration 326: loss = 2935.3563854461563\n",
      "Iteration 327: loss = 2934.6323745002164\n",
      "Iteration 328: loss = 2933.9224454994865\n",
      "Iteration 329: loss = 2933.2263204240508\n",
      "Iteration 330: loss = 2932.543726768451\n",
      "Iteration 331: loss = 2931.8743974319796\n",
      "Iteration 332: loss = 2931.2180706111685\n",
      "Iteration 333: loss = 2930.5744896944075\n",
      "Iteration 334: loss = 2929.9434031586743\n",
      "Iteration 335: loss = 2929.3245644683225\n",
      "Iteration 336: loss = 2928.7177319758794\n",
      "Iteration 337: loss = 2928.1226688248307\n",
      "Iteration 338: loss = 2927.5391428543385\n",
      "Iteration 339: loss = 2926.966926505855\n",
      "Iteration 340: loss = 2926.4057967316066\n",
      "Iteration 341: loss = 2925.855534904893\n",
      "Iteration 342: loss = 2925.3159267321794\n",
      "Iteration 343: loss = 2924.78676216694\n",
      "Iteration 344: loss = 2924.2678353252154\n",
      "Iteration 345: loss = 2923.758944402855\n",
      "Iteration 346: loss = 2923.2598915944104\n",
      "Iteration 347: loss = 2922.770483013642\n",
      "Iteration 348: loss = 2922.2905286156106\n",
      "Iteration 349: loss = 2921.8198421203188\n",
      "Iteration 350: loss = 2921.35824093788\n",
      "Iteration 351: loss = 2920.9055460951718\n",
      "Iteration 352: loss = 2920.461582163954\n",
      "Iteration 353: loss = 2920.0261771904184\n",
      "Iteration 354: loss = 2919.59916262614\n",
      "Iteration 355: loss = 2919.1803732604058\n",
      "Iteration 356: loss = 2918.769647153893\n",
      "Iteration 357: loss = 2918.366825573662\n",
      "Iteration 358: loss = 2917.9717529294526\n",
      "Iteration 359: loss = 2917.584276711241\n",
      "Iteration 360: loss = 2917.2042474280456\n",
      "Iteration 361: loss = 2916.8315185479514\n",
      "Iteration 362: loss = 2916.4659464393253\n",
      "Iteration 363: loss = 2916.1073903132055\n",
      "Iteration 364: loss = 2915.755712166836\n",
      "Iteration 365: loss = 2915.4107767283253\n",
      "Iteration 366: loss = 2915.0724514024123\n",
      "Iteration 367: loss = 2914.7406062173013\n",
      "Iteration 368: loss = 2914.4151137725667\n",
      "Iteration 369: loss = 2914.0958491880883\n",
      "Iteration 370: loss = 2913.7826900540044\n",
      "Iteration 371: loss = 2913.475516381663\n",
      "Iteration 372: loss = 2913.1742105555495\n",
      "Iteration 373: loss = 2912.878657286165\n",
      "Iteration 374: loss = 2912.5887435638524\n",
      "Iteration 375: loss = 2912.3043586135377\n",
      "Iteration 376: loss = 2912.0253938503647\n",
      "Iteration 377: loss = 2911.7517428362253\n",
      "Iteration 378: loss = 2911.483301237147\n",
      "Iteration 379: loss = 2911.21996678153\n",
      "Iteration 380: loss = 2910.9616392192142\n",
      "Iteration 381: loss = 2910.708220281367\n",
      "Iteration 382: loss = 2910.459613641157\n",
      "Iteration 383: loss = 2910.2157248752255\n",
      "Iteration 384: loss = 2909.9764614259116\n",
      "Iteration 385: loss = 2909.741732564237\n",
      "Iteration 386: loss = 2909.511449353621\n",
      "Iteration 387: loss = 2909.285524614325\n",
      "Iteration 388: loss = 2909.063872888598\n",
      "Iteration 389: loss = 2908.846410406516\n",
      "Iteration 390: loss = 2908.633055052506\n",
      "Iteration 391: loss = 2908.4237263325304\n",
      "Iteration 392: loss = 2908.218345341924\n",
      "Iteration 393: loss = 2908.0168347338777\n",
      "Iteration 394: loss = 2907.8191186885406\n",
      "Iteration 395: loss = 2907.6251228827405\n",
      "Iteration 396: loss = 2907.4347744603074\n",
      "Iteration 397: loss = 2907.248002002986\n",
      "Iteration 398: loss = 2907.0647355019246\n",
      "Iteration 399: loss = 2906.884906329736\n",
      "Iteration 400: loss = 2906.70844721311\n",
      "Iteration 401: loss = 2906.5352922059687\n",
      "Iteration 402: loss = 2906.365376663162\n",
      "Iteration 403: loss = 2906.1986372146794\n",
      "Iteration 404: loss = 2906.0350117403805\n",
      "Iteration 405: loss = 2905.8744393452216\n",
      "Iteration 406: loss = 2905.7168603349783\n",
      "Iteration 407: loss = 2905.562216192453\n",
      "Iteration 408: loss = 2905.4104495541505\n",
      "Iteration 409: loss = 2905.261504187418\n",
      "Iteration 410: loss = 2905.115324968044\n",
      "Iteration 411: loss = 2904.971857858298\n",
      "Iteration 412: loss = 2904.8310498854057\n",
      "Iteration 413: loss = 2904.692849120456\n",
      "Iteration 414: loss = 2904.5572046577267\n",
      "Iteration 415: loss = 2904.4240665944158\n",
      "Iteration 416: loss = 2904.2933860107837\n",
      "Iteration 417: loss = 2904.1651149506815\n",
      "Iteration 418: loss = 2904.0392064024722\n",
      "Iteration 419: loss = 2903.915614280329\n",
      "Iteration 420: loss = 2903.7942934059065\n",
      "Iteration 421: loss = 2903.675199490369\n",
      "Iteration 422: loss = 2903.5582891167896\n",
      "Iteration 423: loss = 2903.443519722885\n",
      "Iteration 424: loss = 2903.3308495841\n",
      "Iteration 425: loss = 2903.220237797029\n",
      "Iteration 426: loss = 2903.1116442631624\n",
      "Iteration 427: loss = 2903.0050296729582\n",
      "Iteration 428: loss = 2902.900355490233\n",
      "Iteration 429: loss = 2902.7975839368555\n",
      "Iteration 430: loss = 2902.6966779777513\n",
      "Iteration 431: loss = 2902.5976013062013\n",
      "Stopped early at iteration 431, loss=2902.5976013062013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([151.73584756,   1.85680004, -11.20379518,  25.99665468,\n",
       "        16.38417941,  -4.4851279 ,  -6.12345354, -10.24947679,\n",
       "         6.96975776,  19.51203318,   3.59362696])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "d9e8c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48547e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred = lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "9e06dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mse = mean_absolute_error(y_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7a3f4b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.483503523980396"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e59bf540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353,)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "8ee15963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353, 10)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "562be95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89,)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "7c909ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_absolute_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "88daea13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.069807274251424"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c1867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myConda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
